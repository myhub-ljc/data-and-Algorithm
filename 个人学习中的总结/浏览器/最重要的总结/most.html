<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>浏览器相关最常考的</title>
</head>
<body>
  <!--
    001,HTTPS为什么让数据传输更安全？
    谈到HTTPS, 就不得不谈到与之相对的HTTP。HTTP的特性是明文传输，因此在传输的每一个环节，数据都有可能被第三方窃取或者篡改，具体来说，HTTP 数据经过 TCP 层，然后经过WIFI路由器、运营商和目标服务器，这些环节中都可能被中间人拿到数据并进行篡改，也就是我们常说的中间人攻击。

    为了防范这样一类攻击，我们不得已要引入新的加密方案，即 HTTPS。

    HTTPS并不是一个新的协议, 而是一个加强版的HTTP。其原理是在HTTP和TCP之间建立了一个中间层，当HTTP和TCP通信时并不是像以前那样直接通信，直接经过了一个中间层进行加密，将加密后的数据包传给TCP, 响应的，TCP必须将数据包解密，才能传给上面的HTTP。这个中间层也叫安全层。安全层的核心就是对数据加解密。

    接下来我们就来剖析一下HTTPS的加解密是如何实现的

    首先需要理解对称加密和非对称加密的概念，然后讨论两者应用后的效果如何。
    对称加密是最简单的方式，指的是加密和解密用的是同样的密钥。
    而对于非对称加密，如果有 A、 B 两把密钥，如果用 A 加密过的数据包只能用 B 解密，反之，如果用 B 加密过的数据包只能用 A 解密。

    接着我们来谈谈浏览器和服务器进行协商加解密的过程。
    首先，浏览器会给服务器发送一个随机数client_random和一个加密的方法列表。
    服务器接收后给浏览器返回另一个随机数server_random和加密方法。
    现在，两者拥有三样相同的凭证: client_random、server_random和加密方法。
    接着用这个加密方法将两个随机数混合起来生成密钥，这个密钥就是浏览器和服务端通信的暗号。

    如果用对称加密的方式，那么第三方可以在中间获取到client_random、server_random和加密方法，由于这个加密方法同时可以解密，所以中间人可以成功对暗号进行解密，拿到数据，很容易就将这种加密方式破解了。

    既然对称加密这么不堪一击，我们就来试一试非对称加密。在这种加密方式中，服务器手里有两把钥匙，一把是公钥，也就是说每个人都能拿到，是公开的，另一把是私钥，这把私钥只有服务器自己知道。
    好，现在开始传输。
    浏览器把client_random和加密方法列表传过来，服务器接收到，把server_random、加密方法和公钥传给浏览器。
    现在两者拥有相同的client_random、server_random和加密方法。然后浏览器用公钥将client_random和server_random加密，生成与服务器通信的暗号。
    这时候由于是非对称加密，公钥加密过的数据只能用私钥解密，因此中间人就算拿到浏览器传来的数据，由于他没有私钥，照样无法解密，保证了数据的安全性。

    这难道一定就安全吗？聪明的小伙伴早就发现了端倪。回到非对称加密的定义，公钥加密的数据可以用私钥解密，那私钥加密的数据也可以用公钥解密呀！

    服务器的数据只能用私钥进行加密(因为如果它用公钥那么浏览器也没法解密啦)，中间人一旦拿到公钥，那么就可以对服务端传来的数据进行解密了，就这样又被破解了。而且，只是采用非对称加密，对于服务器性能的消耗也是相当巨大的，因此我们暂且不采用这种方案。

    可以发现，对称加密和非对称加密，单独应用任何一个，都会存在安全隐患。那我们能不能把两者结合，进一步保证安全呢？

    其实是可以的，演示一下整个流程：
    浏览器向服务器发送client_random和加密方法列表。
    服务器接收到，返回server_random、加密方法以及公钥。
    浏览器接收，接着生成另一个随机数pre_random, 并且用公钥加密，传给服务器。(敲黑板！重点操作！)
    服务器用私钥解密这个被加密后的pre_random。

    现在浏览器和服务器有三样相同的凭证:client_random、server_random和pre_random。然后两者用相同的加密方法混合这三个随机数，生成最终的密钥。

    然后浏览器和服务器尽管用一样的密钥进行通信，即使用对称加密。
    回头比较一下和单纯的使用非对称加密, 这种方式做了什么改进呢？本质上是防止了私钥加密的数据外传。单独使用非对称加密，最大的漏洞在于服务器传数据给浏览器只能用私钥加密，这是危险产生的根源。利用对称和非对称加密结合的方式，就防止了这一点，从而保证了安全。

    尽管通过两者加密方式的结合，能够很好地实现加密传输，但实际上还是存在一些问题。黑客如果采用 DNS 劫持，将目标地址替换成黑客服务器的地址，然后黑客自己造一份公钥和私钥，照样能进行数据传输。而对于浏览器用户而言，他是不知道自己正在访问一个危险的服务器的。

    事实上HTTPS在上述结合对称和非对称加密的基础上，又添加了数字证书认证的步骤。其目的就是让服务器证明自己的身份。

    这个数字证书有两个作用:
    服务器向浏览器证明自己的身份。
    把公钥传给浏览器。

    这个验证的过程发生在什么时候呢？
    当服务器传送server_random、加密方法的时候，顺便会带上数字证书(包含了公钥), 接着浏览器接收之后就会开始验证数字证书。如果验证通过，那么后面的过程照常进行，否则拒绝执行。

    浏览器拿到数字证书后，如何来对证书进行认证呢？
    首先，会读取证书中的明文内容。CA 进行数字证书的签名时会保存一个 Hash 函数，来这个函数来计算明文内容得到信息A，然后用公钥解密明文内容得到信息B，两份信息做比对，一致则表示认证合法。

    当然有时候对于浏览器而言，它不知道哪些 CA 是值得信任的，因此会继续查找 CA 的上级 CA，以同样的信息比对方式验证上级 CA 的合法性。一般根级的 CA 会内置在操作系统当中，当然如果向上找没有找到根级的 CA，那么将被视为不合法。

    HTTPS并不是一个新的协议, 它在HTTP和TCP的传输中建立了一个安全层，利用对称加密和非对称加密结合数字证书认证的方式，让传输过程的安全性大大提高。

    002,能不能说一下XSS攻击？
    XSS 全称是 Cross Site Scripting(即跨站脚本)，为了和 CSS 区分，故叫它XSS。XSS 攻击是指浏览器中执行恶意脚本(无论是跨域还是同域)，从而拿到用户的信息并进行操作。

    这些操作一般可以完成下面这些事情:
    窃取Cookie。
    修改 DOM 伪造登录表单。
    监听用户行为，比如输入账号密码后直接发送到黑客服务器。   
    在页面中生成浮窗广告。
    通常情况，XSS 攻击的实现有三种方式——存储型、反射型和文档型。

    存储型，顾名思义就是将恶意脚本存储了起来，确实，存储型的 XSS 将脚本存储到了服务端的数据库，然后在客户端执行这些脚本，从而达到攻击的效果。
    常见的场景是留言评论区提交一段脚本代码，如果前后端没有做好转义的工作，那评论内容存到了数据库，在页面渲染过程中直接执行, 相当于执行一段未知逻辑的 JS 代码，是非常恐怖的。这就是存储型的 XSS 攻击。 

    反射型XSS指的是将恶意脚本作为网络请求的一部分。
    这杨，在服务器端会拿到q参数,然后将内容返回给浏览器端，浏览器将这些内容作为HTML的一部分解析，发现是一个脚本，直接执行，这样就被攻击了。
    之所以叫它反射型, 是因为恶意脚本是通过作为网络请求的参数，经过服务器，然后再反射到HTML文档中，执行解析。和存储型不一样的是，服务器并不会存储这些恶意脚本。

    文档型的 XSS 攻击并不会经过服务端，而是作为中间人的角色，在数据传输过程劫持到网络数据包，然后修改里面的 html 文档！
    这样的劫持方式包括WIFI路由器劫持或者本地恶意软件等。

    明白了三种XSS攻击的原理，我们能发现一个共同点: 都是让恶意脚本直接能在浏览器中执行。
    那么要防范它，就是要避免这些脚本代码在浏览器的执行。
    为了完成这一点，必须做到一个信念，两个利用。
    无论是在前端和服务端，都要对用户的输入进行转码或者过滤。

    CSP，即浏览器中的内容安全策略，它的核心思想就是服务器决定浏览器加载哪些资源，具体来说可以完成以下功能:
    限制其他域下的资源加载。
    禁止向其它域提交数据。
    提供上报机制，能帮助我们及时发现 XSS 攻击。

    很多 XSS 攻击脚本都是用来窃取Cookie, 而设置 Cookie 的 HttpOnly 属性后，JavaScript 便无法读取 Cookie 的值。这样也能很好的防范 XSS 攻击。

    XSS 攻击是指浏览器中执行恶意脚本, 然后拿到用户的信息进行操作。主要分为存储型、反射型和文档型。防范的措施包括:

    一个信念: 不要相信用户的输入，对输入内容转码或者过滤，让其不可执行。
    两个利用: 利用 CSP，利用 Cookie 的 HttpOnly 属性。

    003,能不能说一说CORF攻击？
    CSRF(Cross-site request forgery), 即跨站请求伪造，指的是黑客诱导用户点击链接，打开黑客的网站，然后黑客利用用户目前的登录状态发起跨站请求。

    接下来我们就来拆解一下当你点击了链接之后，黑客在背后做了哪些事情。
    可能会做三样事情。列举如下：
    ·自动发送get请求
    进入页面后自动发送 get 请求，值得注意的是，这个请求会自动带上关于 xxx.com 的 cookie 信息(这里是假定你已经在 xxx.com 中登录过)。
    假如服务器端没有相应的验证机制，它可能认为发请求的是一个正常的用户，因为携带了相应的 cookie，然后进行相应的各种操作，可以是转账汇款以及其他的恶意操作。

    ·自动发送post请求
    黑客可能自己填了一个表单，写了一段自动提交的脚本。
    同样也会携带相应的用户 cookie 信息，让服务器误以为是一个正常的用户在操作，让各种恶意的操作变为可能。
    
    这就是CSRF攻击的原理。和XSS攻击对比，CSRF并不会写入恶意脚本代码，而是跳转到新的页面，利用服务器的验证漏洞以及用户之前的登陆状态来模拟真实用户进行操作。

    那么如何进行防范呢
    1，利用cookie的samesite属性
    CSRF攻击中重要的一环就是自动发送目标站点下的 Cookie,然后就是这一份 Cookie 模拟了用户的身份。因此在Cookie上面下文章是防范的不二之选。

    恰好，在 Cookie 当中有一个关键的字段，可以对请求中 Cookie 的携带作一些限制，这个字段就是SameSite。

    SameSite可以设置为三个值，Strict、Lax和None。
     在Strict模式下，浏览器完全禁止第三方请求携带Cookie。比如请求sanyuan.com网站只能在sanyuan.com域名当中请求才能携带 Cookie，在其他网站请求都不能。
     在Lax模式，就宽松一点了，但是只能在 get 方法提交表单况或者a 标签发送 get 请求的情况下可以携带 Cookie，其他情况均不能。
     在None模式下，也就是默认模式，请求会自动携带上 Cookie。

    2，CORF TOKEN
    首先，浏览器向服务器发送请求时，服务器生成一个字符串，将其植入到返回的页面中。
    然后浏览器如果要发送请求，就必须带上这个字符串，然后服务器来验证是否合法，如果不合法则不予响应。这个字符串也就是CSRF Token，通常第三方站点无法拿到这个 token, 因此也就是被服务器给拒绝。

    004，说说你对回流和重绘的理解
    1，回流
    简单来说，就是当我们对 DOM 结构的修改引发 DOM 几何尺寸变化的时候，会发生回流的过程。
    具体一点，有以下的操作会触发回流:
    ·一个 DOM 元素的几何属性变化，常见的几何属性有width、height、padding、margin、left、top、border 等等, 这个很好理解。
    ·使 DOM 节点发生增减或者移动。
    ·读写 offset族、scroll族和client族属性的时候，浏览器为了获取这些值，需要进行回流操作。

    依照上面的渲染流水线，触发回流的时候，如果 DOM 结构发生改变，则重新渲染 DOM 树，相当于将解析和合成的过程重新又走了一篇，开销是非常大的。

    2，重绘
    当 DOM 的修改导致了样式的变化，并且没有影响几何属性的时候，会导致重绘(repaint)。
    可以看到，重绘不一定导致回流，但回流一定发生了重绘。

    3，合成
    还有一种情况，是直接合成。比如利用 CSS3 的transform、opacity、filter这些属性就可以实现合成的效果，也就是大家常说的GPU加速。

    那么GPU加速的原因是？
    在合成的情况下，会直接进入合成线程处理，交给他们处理有两个好处。
    ·能够充分发挥GPU的优势。合成线程生成位图的过程中会调用线程池，并在其中使用GPU进行加速生成，而GPU 是擅长处理位图数据的。
    ·没有占用主线程的资源，即使主线程卡住了，效果也能依然流畅的展示。

    知道上面的原理之后，对于开发过程有什么指导意义呢？
    避免频繁使用 style，而是采用修改class的方式。
    使用createDocumentFragment进行批量的 DOM 操作。
    对于 resize、scroll 等进行防抖/节流处理。
    添加 will-change: tranform ，让渲染引擎为其单独实现一个图层，当这些变换发生时，仅仅只是利用合成线程去处理这些变换，而不牵扯到主线程，大大提高渲染效率。当然这个变化不限于tranform, 任何可以实现合成效果的 CSS 属性都能用will-change来声明。

    005，能不能说一下浏览器缓存？
    缓存是性能优化中非常重要的一环，浏览器的缓存机制对开发也是非常重要的知识点。接下来以三个部分来把浏览器的缓存机制说清楚：
    ·强缓存
    ·协商缓存
    ·缓存位置

    1，强缓存
    浏览器中的缓存作用分为两种情况，一种是需要发送HTTP请求，一种是不需要发送。
    首先是检查强缓存，这个阶段不需要发送HTTP请求。
    如何来检查呢？通过相应的字段来进行，但是说起这个字段就有点门道了。
    在HTTP/1.0和HTTP/1.1当中，这个字段是不一样的。在早期，也就是HTTP/1.0时期，使用的是Expires，而HTTP/1.1使用的是Cache-Control。让我们首先来看看Expires。
    
    Expires即过期时间，存在于服务端返回的响应头中，告诉浏览器在这个过期时间之前可以直接从缓存里面获取数据，无需再次请求。
    这个方式看上去没什么问题，合情合理，但其实潜藏了一个坑，那就是服务器的时间和浏览器的时间可能并不一致，那服务器返回的这个过期时间可能就是不准确的。因此这种方式很快在后来的HTTP1.1版本中被抛弃了。

    在HTTP1.1中，采用了一个非常关键的字段：Cache-Control。
    它和Expires本质的不同在于它并没有采用具体的过期时间点这个方式，而是采用过期时长来控制缓存，对应的字段是max-age。
    如果你觉得它只有max-age一个属性的话，那就大错特错了。
    它其实可以组合非常多的指令，完成更多场景的缓存判断, 将一些关键的属性列举如下:
    public: 客户端和代理服务器都可以缓存。因为一个请求可能要经过不同的代理服务器最后才到达目标服务器，那么结果就是不仅仅浏览器可以缓存数据，中间的任何代理节点都可以进行缓存。
    private： 这种情况就是只有浏览器能缓存了，中间的代理服务器不能缓存。
    no-cache: 跳过当前的强缓存，发送HTTP请求，即直接进入协商缓存阶段。
    no-store：非常粗暴，不进行任何形式的缓存。
    s-maxage：这和max-age长得比较像，但是区别在于s-maxage是针对代理服务器的缓存时间。

    值得注意的是，当Expires和Cache-Control同时存在的时候，Cache-Control会优先考虑。

    当然，还存在一种情况，当资源缓存时间超时了，也就是强缓存失效了，接下来怎么办？没错，这样就进入到第二级屏障——协商缓存了。

    2，协商缓存
    强缓存失效之后，浏览器在请求头中携带相应的缓存tag来向服务器发请求，由服务器根据这个tag，来决定是否使用缓存，这就是协商缓存。
    具体来说，这样的缓存tag分为两种: Last-Modified 和 ETag。这两者各有优劣，并不存在谁对谁有绝对的优势，跟上面强缓存的两个 tag 不一样。

    Last-Modified
    即最后修改时间。在浏览器第一次给服务器发送请求后，服务器会在响应头中加上这个字段。
    浏览器接收到后，如果再次请求，会在请求头中携带If-Modified-Since字段，这个字段的值也就是服务器传来的最后修改时间。
    服务器拿到请求头中的If-Modified-Since的字段后，其实会和这个服务器中该资源的最后修改时间对比:

    如果请求头中的这个值小于最后修改时间，说明是时候更新了。返回新的资源，跟常规的HTTP请求响应的流程一样。
    否则返回304，告诉浏览器直接用缓存。

    ETag
    ETag 是服务器根据当前文件的内容，给文件生成的唯一标识，只要里面的内容有改动，这个值就会变。服务器通过响应头把这个值给浏览器。
    浏览器接收到ETag的值，会在下次请求时，将这个值作为If-None-Match这个字段的内容，并放到请求头中，然后发给服务器。
    服务器接收到If-None-Match后，会跟服务器上该资源的ETag进行比对:

    如果两者不一样，说明要更新了。返回新的资源，跟常规的HTTP请求响应的流程一样。
    否则返回304，告诉浏览器直接用缓存。

    两者对比
    在精准度上，ETag优于Last-Modified。优于 ETag 是按照内容给资源上标识，因此能准确感知资源的变化。而 Last-Modified 就不一样了，它在一些特殊的情况并不能准确感知资源变化。
    在性能上，Last-Modified优于ETag，也很简单理解，Last-Modified仅仅只是记录一个时间点，而 Etag需要根据文件的具体内容生成哈希值。
    另外，如果两种方式都支持的话，服务器会优先考虑ETag。

    缓存位置
    前面我们已经提到，当强缓存命中或者协商缓存中服务器返回304的时候，我们直接从缓存中获取资源。那这些资源究竟缓存在什么位置呢？

    浏览器中的缓存位置一共有四种，按优先级从高到低排列分别是：
    ·Service Worker
    ·Memory Cache
    ·Disk Cache
    ·Push Cache

    Service Worker 借鉴了 Web Worker的 思路，即让 JS 运行在主线程之外，由于它脱离了浏览器的窗体，因此无法直接访问DOM。虽然如此，但它仍然能帮助我们完成很多有用的功能，比如离线缓存、消息推送和网络代理等功能。其中的离线缓存就是 Service Worker Cache。

    Memory Cache指的是内存缓存，从效率上讲它是最快的。但是从存活时间来讲又是最短的，当渲染进程结束后，内存缓存也就不存在了。
    Disk Cache就是存储在磁盘中的缓存，从存取效率上讲是比内存缓存慢的，但是他的优势在于存储容量和存储时长。稍微有些计算机基础的应该很好理解，就不展开了。
    好，现在问题来了，既然两者各有优劣，那浏览器如何决定将资源放进内存还是硬盘呢？主要策略如下：

    比较大的JS、CSS文件会直接被丢进磁盘，反之丢进内存
    内存使用率比较高的时候，文件优先进入磁盘

    即推送缓存，这是浏览器缓存的最后一道防线。它是 HTTP/2 中的内容，虽然现在应用的并不广泛，但随着 HTTP/2 的推广，它的应用越来越广泛。

    对浏览器的缓存机制来做个简要的总结:
    首先通过 Cache-Control 验证强缓存是否可用

    如果强缓存可用，直接使用
    否则进入协商缓存，即发送 HTTP 请求，服务器通过请求头中的If-Modified-Since或者If-None-Match字段检查资源是否更新

    若资源更新，返回资源和200状态码
    否则，返回304，告诉浏览器直接从缓存获取资源

    006，能不能说一下浏览器的本地存储？以及各自的优劣？
    浏览器本地存储主要分为cookie，localstorage和sessionStorage；
    
    cookie
    Cookie 最开始被设计出来其实并不是来做本地存储的，而是为了弥补HTTP在状态管理上的不足。

    HTTP 协议是一个无状态协议，客户端向服务器发请求，服务器返回响应，故事就这样结束了，但是下次发请求如何让服务端知道客户端是谁呢？

    这种背景下，就产生了 Cookie.
    Cookie 本质上就是浏览器里面存储的一个很小的文本文件，内部以键值对的方式来存储(在chrome开发者面板的Application这一栏可以看到)。向同一个域名下发送请求，都会携带相同的 Cookie，服务器拿到 Cookie 进行解析，便能拿到客户端的状态。

    Cookie 的作用很好理解，就是用来做状态存储的，但它也是有诸多致命的缺陷的：
    ·容量缺陷。Cookie 的体积上限只有4KB，只能用来存储少量的信息。
    ·性能缺陷。Cookie 紧跟域名，不管域名下面的某一个地址需不需要这个 Cookie ，请求都会携带上完整的 Cookie，这样随着请求数的增多，其实会造成巨大的性能浪费的，因为请求携带了很多不必要的内容。
    ·安全缺陷。由于 Cookie 以纯文本的形式在浏览器和服务器中传递，很容易被非法用户截获，然后进行一系列的篡改，在 Cookie 的有效期内重新发送给服务器，这是相当危险的。另外，在HttpOnly为 false 的情况下，Cookie 信息能直接通过 JS 脚本来读取。

    localStorage
    localStorage有一点跟Cookie一样，就是针对一个域名，即在同一个域名下，会存储相同的一段localStorage。

    不过它相对Cookie还是有相当多的区别的:
    容量。localStorage 的容量上限为5M，相比于Cookie的 4K 大大增加。当然这个 5M 是针对一个域名的，因此对于一个域名是持久存储的。

    只存在客户端，默认不参与与服务端的通信。这样就很好地避免了 Cookie 带来的性能问题和安全问题。
    从这里可以看出，localStorage其实存储的都是字符串，如果是存储对象需要调用JSON的stringify方法，并且用JSON.parse来解析成对象。

    利用localStorage的较大容量和持久特性，可以利用localStorage存储一些内容稳定的资源，比如官网的logo，存储Base64格式的图片资源，因此利用localStorage

    sessionStorage
    sessionStorage以下方面和localStorage一致:
    容量。容量上限也为 5M。
    只存在客户端，默认不参与与服务端的通信。
    接口封装。除了sessionStorage名字有所变化，存储方式、操作方式均和localStorage一样。

    但sessionStorage和localStorage有一个本质的区别，那就是前者只是会话级别的存储，并不是持久化存储。会话结束，也就是页面关闭，这部分sessionStorage就不复存在了。
    可以用它对表单信息进行维护，将表单信息存储在里面，可以保证页面即使刷新也不会让之前的表单信息丢失。
    可以用它存储本次浏览记录。如果关闭页面后不需要这些记录，用sessionStorage就再合适不过了。事实上微博就采取了这样的存储方式。

    浏览器中各种本地存储和缓存技术的发展，给前端应用带来了大量的机会，PWA 也正是依托了这些优秀的存储方案才得以发展起来。重新梳理一下这些本地存储方案:

    cookie并不适合存储，而且存在非常多的缺陷。
    Web Storage包括localStorage和sessionStorage, 默认不会参与和服务器的通信。

    007，让你更好的去回答关于tcp的问题
    1，三次握手
    三次握手（Three-way Handshake）其实就是指建立一个TCP连接时，需要客户端和服务器总共发送3个包。进行三次握手的主要作用就是为了确认双方的接收能力和发送能力是否正常、指定自己的初始化序列号为后面的可靠性传送做准备。

    刚开始客户端处于 Closed 的状态，服务端处于 Listen 状态。

    进行三次握手：

    客户端主动发起连接，发送 SYN 给服务器, 自己变成了SYN-SENT状态。
    服务端接收到，返回SYN和ACK(对应客户端发来的SYN)，自己变成了SYN-REVD状态。
    之后客户端再发送ACK给服务端，自己变成了ESTABLISHED状态；服务端收到ACK之后，也变成了ESTABLISHED状态。

    另外需要提醒你注意的是，SYN 是需要消耗一个序列号的，下次发送对应的 ACK 序列号要加1，因为凡是需要对端确认的，都会消耗TCP报文的一个序列号。(但是需要注意SYN需要对端的确认，而 ACK 并不需要，因此 SYN 消耗一个序列号而 ACK 不需要。)

    为什么不是两次握手？
    根本原因: 无法确认客户端的接收能力。
    如果是两次，你现在发了 SYN 报文想握手，但是这个包滞留在了当前的网络中迟迟没有到达，TCP 以为这是丢了包，于是重传，两次握手建立好了连接。
    看似没有问题，但是连接关闭后，如果这个滞留在网路中的包到达了服务端呢？这时候由于是两次握手，服务端只要接收到然后发送相应的数据包，就默认建立连接，但是现在客户端已经断开了。
    看到问题的吧，这就带来了连接资源的浪费。

    为什么不是四次？
    三次握手的目的是确认双方发送和接收的能力，那四次握手可以嘛？
    当然可以，100 次都可以。但为了解决问题，三次就足够了，再多用处就不大了。

    三次握手过程中可以携带数据么？
    第三次握手的时候，可以携带。前两次握手不能携带数据。
    如果前两次握手能够携带数据，那么一旦有人想攻击服务器，那么他只需要在第一次握手中的 SYN 报文中放大量数据，那么服务器势必会消耗更多的时间和内存空间去处理这些数据，增大了服务器被攻击的风险。
    第三次握手的时候，客户端已经处于ESTABLISHED状态，并且已经能够确认服务器的接收、发送能力正常，这个时候相对安全了，可以携带数据。

    2，四次挥手
    刚开始双方处于ESTABLISHED状态。

    客户端要断开了，向服务器发送 FIN 报文，发送后客户端变成了FIN-WAIT-1状态；
    服务端接收后向客户端确认，变成了CLOSED-WAIT状态。
    客户端接收到了服务端的确认，变成了FIN-WAIT2状态。
    随后，服务端向客户端发送FIN，自己进入LAST-ACK状态，
    客户端收到服务端发来的FIN后，自己变成了TIME-WAIT状态，然后发送 ACK 给服务端。
    注意了，这个时候，客户端需要等待足够长的时间，具体来说，是 2 个 MSL(Maximum Segment Lifetime，报文最大生存时间), 在这段时间内如果客户端没有收到服务端的重发请求，那么表示 ACK 成功到达，挥手结束，否则客户端重发 ACK。

    如果不等待2msl时间会怎样？
    为了保证客户端发送的最后一个ACK报文段能够到达服务器。因为这个ACK有可能丢失，从而导致处在LAST-ACK状态的服务器收不到对FIN-ACK的确认报文。服务器会超时重传这个FIN-ACK，接着客户端再重传一次确认，重新启动时间等待计时器。最后客户端和服务器都能正常的关闭。假设客户端不等待2MSL，而是在发送完ACK之后直接释放关闭，一但这个ACK丢失的话，服务器就无法正常的进入关闭连接状态。
    ·保证ACK成功到达服务器
    ·保证已经失效的请求报文段再次出现在本链接中。

    
  -->
</body>
</html>